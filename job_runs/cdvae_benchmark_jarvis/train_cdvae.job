#!/bin/bash

#SBATCH --job-name=cdvae_tc            
#SBATCH --output=/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/cdvae_benchmark_jarvis/fl_studio.out
#SBATCH --partition=batch
#SBATCH --nodes=1                         
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=4                # Run 4 tasks (processes) on the node
#SBATCH --cpus-per-task=2                  # 2 CPU cores per task (for multi‚Äêthreaded code)
#SBATCH --time=24:00:00                    # Max walltime (HH:MM:SS)
#SBATCH --mem=64G                           # Total RAM for the job (8 GB)

module load cuda/11.8
source /home/crc8/miniconda3/etc/profile.d/conda.sh
conda activate cdvae
nvidia-smi



export PROJECT_ROOT="/lab/mml/kipp/677/jarvis/rhys/benchmarks/models/cdvae"
export HYDRA_JOBS="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/cdvae_benchmark_jarvis/hydra_outputs"
export HYDRA_FULL_ERROR="1"
export WABDB_DIR="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/cdvae_benchmark_jarvis/wandb_outputs"
#export WANDB_MODE="offline"
source wandb_api_key.sh

python -u -m cdvae.run data=supercon expname=supercon \
	model.num_noise_level=2 \
	data.train_max_epochs=1 \
	train.pl_trainer.gpus=1 \
	model.predict_property=True \

echo "Done"



	
