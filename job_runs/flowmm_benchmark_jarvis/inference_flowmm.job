#!/bin/bash

#SBATCH --job-name=flow_jarv
#SBATCH --output=/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/flowmm_benchmark_jarvis/fl_studio.out
#SBATCH --partition=batch
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=4                # Run 4 tasks (processes) on the node
#SBATCH --cpus-per-task=2                  # 2 CPU cores per task (for multi‚Äêthreaded code)
#SBATCH --time=24:00:00                    # Max walltime (HH:MM:SS)
#SBATCH --mem=64G                          # Total RAM for the job (8 GB)

# setup
source /home/crc8/miniconda3/etc/profile.d/conda.sh
source wandb_api_key.sh
module load cuda/11.8
conda activate flowmm
export HYDRA_FULL_ERROR="1"
export WABDB_DIR="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/flowmm_benchmark_jarvis/outputs/wandb_outputs"
export FLOWMM_RUN_ROOT="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/flowmm_benchmark_jarvis/outputs/"
export PATH_TO_CHECKPOINT="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/flowmm_benchmark_jarvis/outputs/rfmcsp-conditional-supercon/zjnmz1mi/checkpoints/epoch=8-step=900.ckpt"
export NAME_OF_SUBDIRECTORY_AT_CHECKPOINT="/lab/mml/kipp/677/jarvis/rhys/benchmarks/job_runs/flowmm_benchmark_jarvis/outputs/rfmcsp-conditional-supercon/zjnmz1mi/checkpoints/inferences"
export SLOPE_OF_INFERENCE_ANTI_ANNEALING="2.0"
ckpt="$PATH_TO_CHECKPOINT"
subdir="$NAME_OF_SUBDIRECTORY_AT_CHECKPOINT"
slope="$SLOPE_OF_INFERENCE_ANTI_ANNEALING"

# commands
nvidia-smi
cd /lab/mml/kipp/677/jarvis/rhys/benchmarks/models/flowmm/
bash /lab/mml/kipp/677/jarvis/rhys/benchmarks/models/flowmm/create_env_file.sh
python scripts_model/evaluate.py reconstruct ${ckpt} --subdir ${subdir} --inference_anneal_slope ${slope} --stage test && \
python scripts_model/evaluate.py consolidate ${ckpt} --subdir ${subdir}

echo "Done"



